version: "2.1"
services:

  neo4j:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/neo4j:3.4.0
    container_name: hubble-neo4j
    restart: always
    network_mode: "host"
    volumes:
      - /etc/localtime:/etc/localtime
      - ./data/neo4j/neo4jdata:/data
    environment:
      - NEO4J_AUTH=none

  dataquality:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/dataquality:{{ HUBBLE_IMAGE_VERSION }}
    container_name: hubble-dataquality
    restart: always
    network_mode: "host"
    healthcheck:
      test: curl -f http://localhost:16111/health || exit 1
      interval: 1m30s
      timeout: 10s
    volumes:
      - /etc/hosts:/etc/hosts
      - /etc/hive/conf:/opt/server/metagrid/dataquality/spark/hive_conf
      - /etc/localtime:/etc/localtime
      - ./data/dataquality/tmp:/tmp
      - ./data/dataquality/jars:/user/chinacloud/dataquality/
      - ./data/dataquality/logs:/opt/server/metagrid/dataquality/spark/logs
      - ./data/dataquality/application.properties:/opt/server/metagrid/dataquality/conf/rs/application.properties
      - ./data/dataquality/dataquality.jar:/opt/server/metagrid/dataquality/dataquality.jar
      - ./data/java.security:/opt/jdk/jre/lib/security/java.security
    env_file:
      - global.env
    environment:
      - port=16111
      - application_name=hubble-dq
      - db_name=metagrid
      - db_user=root
      - db_pwd=chinacloudroot
      - spark_master=yarn
      - spark_local_dir_clean_schedule=0 0 2 * * ?
      - spark_local_dir_clean_day=2
      - spark_standalone_url=spark://slave3.huacloud.test:6066
      - super_tenant=512032de-e220-45ba-bd23-1f15cd3c0f88
      - job_merge_size=10

  ureport:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/ureport:release2.1.3
    container_name: hubble-ureport
    restart: always
    mem_limit: 1024m
    mem_reservation: 400m
    ports:
      - 16667:16667
    network_mode: "host"
    healthcheck:
      test: curl -f http://localhost:16667/health || exit 1
      interval: 1m30s
      timeout: 10s
    volumes:
      - /etc/localtime:/etc/localtime
      - ./data/upload/ureportfiles:/upload/ureportfiles
    env_file:
      -  global.env
    environment:
      - port=16667


  logstash:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/logstash:2.3
    container_name: hubble-logstash
    restart: always
    network_mode: "host"
    healthcheck:
      test: exit 0
      interval: 30s
      timeout: 10s
    volumes:
      - /etc/localtime:/etc/localtime
    env_file:
      -  global.env
    environment:
      - port=4563
      - db_name=one_log
      - db_user=onelog
      - db_pwd=chinacloudonelog

  elasticsearch:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/elasticsearch:2.3
    container_name: hubble-elasticsearch
    restart: always
    network_mode: "host"
    healthcheck:
      test: curl -f http://localhost:9200/_cat || exit 1
      interval: 1m30s
      timeout: 10s
    volumes:
      - /etc/localtime:/etc/localtime
      - ./data/elasticsearch/esdata:/usr/share/elasticsearch/data
      - ./data/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    env_file:
      - global.env
    environment:
      - ES_JAVA_OPTS=-Xms512m -Xmx2048m


  jobtracker:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/jobtracker:1.6.8
    container_name: hubble-jobtracker
    restart: always
    network_mode: "host"
    volumes:
      - /etc/localtime:/etc/localtime
    env_file:
      -  global.env
    environment:
      - db_name=orchestra
      - db_user=orcadmin
      - db_pwd=chinacloudorcadmin


  datasync:
    image: {{ docker_registry }}/{{ HUBBLE_IMAGE_GROUP }}/datasync:{{ HUBBLE_IMAGE_VERSION }}
    container_name: hubble-datasync
    restart: always
    network_mode: "host"
    healthcheck:
      test: curl -f http://localhost:5678/health || exit 1
      interval: 1m30s
      timeout: 10s
    env_file:
      - global.env
    volumes:
      - /etc/hadoop:/etc/hadoop
      - /etc/hbase:/etc/hbase
      - /etc/hive:/etc/hive
      - /etc/zookeeper:/etc/zookeeper
      - /etc/localtime:/etc/localtime
      - /etc/hosts:/etc/hosts
      - ./data/datasync/cdh/jars:/opt/cloudera/parcels/CDH/jars
      - ./data/datasync/cdh/lib/hadoop:/opt/cloudera/parcels/CDH/lib/hadoop
      - ./data/datasync/cdh/lib/hadoop-hdfs:/opt/cloudera/parcels/CDH/lib/hadoop-hdfs
      - ./data/datasync/cdh/lib/hadoop-mapreduce:/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce
      - ./data/datasync/cdh/lib/hadoop-yarn:/opt/cloudera/parcels/CDH/lib/hadoop-yarn
      - ./data/datasync/cdh/lib/zookeeper:/opt/cloudera/parcels/CDH/lib/zookeeper
      - ./data/datasync/cdh/lib/hbase:/opt/cloudera/parcels/CDH/lib/hbase
      - ./data/datasync/cdh/lib/hive:/opt/cloudera/parcels/CDH/lib/hive
      - ./data/datasync/cdh/lib/hive-hcatalog:/opt/cloudera/parcels/CDH/lib/hive-hcatalog
      - /etc/hadoop/conf/hdfs-site.xml:/opt/server/stargate/datasync/conf/rs/hdfs-site.xml
      - /etc/hadoop/conf/mapred-site.xml:/opt/server/stargate/datasync/conf/rs/mapred-site.xml
      - /etc/hadoop/conf/core-site.xml:/opt/server/stargate/datasync/conf/rs/core-site.xml
      - /etc/hadoop/conf/yarn-site.xml:/opt/server/stargate/datasync/conf/rs/yarn-site.xml
      - /etc/hive/conf/hive-site.xml:/opt/server/stargate/datasync/conf/rs/hive-site.xml
      - /etc/hbase/conf/hbase-site.xml:/opt/server/stargate/datasync/conf/rs/hbase-site.xml
      - ./data/datasync/log:/var/log/stargate/datasync
      - ./data/datasync/tmp:/tmp
    environment:
      - port=5678
      - datasync_port=16003
      - datasync_admin_port=16004
      - application_name=hubble-datasync
      - db_user=root
      - db_pwd=chinacloudroot
      - thread_count=5
      - super_tenant=512032de-e220-45ba-bd23-1f15cd3c0f88
      - skip_null=false

